{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMzNyIgjtgaUVsX/56m3oCk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kenneth-craig/academyPub/blob/main/GC-WineQuality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "IvmqnHJqMwMY",
        "outputId": "b010c1db-b910-460d-b855-2de8f39f454a"
      },
      "source": [
        "#### Here is the script to get the data directly from kaggle through colab\n",
        "\n",
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "#create a kaggle folder\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "# Go on kaggle > Account > Create New API token\n",
        "# Save the json file in your laptop in a dedicated folder\n",
        "\n",
        "# copy the kaggle.json to folder created\n",
        "!cp kaggle.json ~/.kaggle\n",
        "#permission for the json to act\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Datasets available here: \n",
        "# https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009\n",
        "!kaggle datasets download -d uciml/red-wine-quality-cortez-et-al-2009\n",
        "!unzip red-wine-quality-cortez-et-al-2009.zip\n",
        "\n",
        "#### End of the script to get the data directly from kaggle through colab\n",
        "# Remember that when you run it, you have to click on the button Choose Files and \n",
        "# then select the kaggle.json file from your computer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4ca57d7c-ec2e-40d1-baa5-4899af94a2c4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4ca57d7c-ec2e-40d1-baa5-4899af94a2c4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_0zWZ8RM8q6"
      },
      "source": [
        "# Start of the Python programming language basics\n",
        "You will be able to understand how Python works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5LKkooXM1JP"
      },
      "source": [
        "##### Python as a programming language basics\n",
        "\n",
        "# The usage of the hashtag represents a comment. everything on the same line and after\n",
        "# will be not take into account\n",
        "\n",
        "# If you execute the content of a cell, only the last element will be displayed\n",
        "7+3\n",
        "7-3\n",
        "7*3\n",
        "21/2\n",
        "21//2 # floor division\n",
        "21%2 # modulo\n",
        "21**2 # exponential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGZqAi8cNOEe"
      },
      "source": [
        "# You will always want to associate a number / a result to a variable:\n",
        "gg=2 #It means that now, Python knows that the value of gg is 2\n",
        "gg==3 #Here, '==' means: Does gg is equal to 3 ? Given that you created a variable just before saying \n",
        "# that gg equals 2, Python answers that it is False because 3 is different than 2\n",
        "# However if you write gg==2, Python will reply \"True\"\n",
        "# Given that you defined the variable gg, if you gg Python will display the value 2."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhsEjzOcNT6n"
      },
      "source": [
        "# Note that you can change the value of gg by defining again the variable gg:\n",
        "gg=3\n",
        "# Instead of asking Python if a variable is equals to a value, you can use inequalities:\n",
        "gg<=3\n",
        "gg>=3\n",
        "gg>3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtXw48wBNUy_"
      },
      "source": [
        "# When you define a variable, it is case sensitive. It means that the 4 names below represent \n",
        "# 4 different names\n",
        "beautiful_variable = 3\n",
        "Beautiful_variable = 4\n",
        "BEAUTIFUL_VARIABLE = 5\n",
        "BeaUtIfUL_vaRiABLe = 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kaA4zvrNXm9"
      },
      "source": [
        "# It exists several data types in Python the most common & used is the list\n",
        "# A list is defined by elements in [] The elements can be both numeric or textual\n",
        "\n",
        "cities = ['Paris', 'Zurich', 'Rome', 'Berlin', 'London']\n",
        "\n",
        "# A list is 0 based in term of index which means that the first element of the list (Paris) is the element 0\n",
        "print(cities[0])\n",
        "# You can access the element by the end:\n",
        "print(cities[-1])\n",
        "\n",
        "# You can modify a list. Here you want to modify the third element (Rome) by Milan:\n",
        "cities[2] = 'Milan'\n",
        "print(cities)\n",
        "\n",
        "# In order to insert an element in a list: list.insert(1, element)\n",
        "# To insert HK between Zurich & Milan\n",
        "cities.insert(2, 'HK')\n",
        "print(cities)\n",
        "\n",
        "# In order to add an element at the end of the list:\n",
        "cities.append('Rio')\n",
        "print(cities)\n",
        "\n",
        "# In order to remove an element from the list:\n",
        "cities.remove('Paris')\n",
        "print(cities)\n",
        "\n",
        "# In this list we only have textual elements, we can add some numerical one:\n",
        "cities.insert(2, 848)\n",
        "print(cities)\n",
        "cities.append(848)\n",
        "print(cities)\n",
        "# You can see that we have 2 values of 848 in our list. The remove function only remove the \n",
        "# first time Python see the value:\n",
        "cities.remove(848)\n",
        "print(cities)\n",
        "# We will see later how to delete all the values 848.\n",
        "\n",
        "# If you want to access the index (= the position) of an element in the list:\n",
        "print(cities.index(\"HK\"))\n",
        "\n",
        "# If you want to concatenate 2 lists:\n",
        "new_list = [1, 3, 5, 2]\n",
        "cities = cities + new_list\n",
        "print(cities)\n",
        "\n",
        "# There are other types than list, such as tuple & dictionnary.\n",
        "# Keep in mind that list are defined by [], if you use (), it will be a tuple."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roe6i_qINcYv"
      },
      "source": [
        "### Conditional statements:\n",
        "# The global idea is to do an action based on a current state\n",
        "val = 0 # Here the current state is that the variable val has the value 0\n",
        "if val > 0:\n",
        "  print('The value is positive')\n",
        "elif val < 0:\n",
        "  print('The value is negative')\n",
        "elif val<-10:\n",
        "  print('very low')\n",
        "else:\n",
        "  print('The value equals 0')\n",
        "\n",
        "# It is very important to understand the chain, on the previous example, if you replace val=0 by \n",
        "# val = -12, the statement will return 'The value is negative' because it fits the condition val<0.\n",
        "# You have to switch val < 0 and val <-10 like this:\n",
        "val=0\n",
        "if val > 0:\n",
        "  print('The value is positive')\n",
        "elif val < -10:\n",
        "  print('The value is very low')\n",
        "elif val<0:\n",
        "  print('The value is negative')\n",
        "else:\n",
        "  print('The value equals 0')\n",
        "\n",
        "# Be careful, the indentation is important !!!\n",
        "\n",
        "# Logical expressions for if:\n",
        "# a == b: is a equal to b\n",
        "# a != b: is a different from b\n",
        "# a <= b: is a lower or equal than b\n",
        "# a >= b: is a greater or equal than b\n",
        "# a < b: is a strictly lower than b\n",
        "# a > b: is a strictly greater than b\n",
        "# a is b: is a the same object as b\n",
        "# condition_a and condition_b: logical AND\n",
        "# condition_a or condition_b: logical OR\n",
        "# not: negation\n",
        "\n",
        "if 7 not in [0, 1, 2, 3, 4]:\n",
        "  print('Indeed')\n",
        "\n",
        "# Of course, in the real life, you will do more advanced condition that just display a print after a condition."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEI_3y1KNhU2"
      },
      "source": [
        "### Loop\n",
        "# while loop: loop until the condition happened -> can run to the infinity\n",
        "# Ex while loop\n",
        "N=15\n",
        "total = 0\n",
        "counter = 1\n",
        "while counter <= N:\n",
        "  total += counter # total = total+counter\n",
        "  counter += 1\n",
        "  print(total)\n",
        "\n",
        "\n",
        "\n",
        "# for loop (most used): loop on all the defined elements of an object.\n",
        "# Ex for loop\n",
        "fruits = ['apple', 'pear', 'cherry']\n",
        "for i in fruits:\n",
        "  print(i)\n",
        "\n",
        "# It is as well possible to do a loop within a loop (a nested loop):\n",
        "list_of_list = [['a', 'b', 'c'], [1, 2, 3]]\n",
        "for sub_list in list_of_list:\n",
        "  print(\"We treat the list: \", sub_list)\n",
        "  for element in sub_list:\n",
        "    print('We display the element: ', element)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc2ybU5gNkeJ"
      },
      "source": [
        "### Functions\n",
        "# Obj: automate an action composed by several instructions\n",
        "# The keyword def introduces the definition of a function\n",
        "# It is followed by the name of the function and a series of parameters in parenthesis\n",
        "# It is possible to define a function without entry parameters\n",
        "# The instructions constituing the body of the function starts at the next line and must have an identation\n",
        "# Instruction return causes the function to be removes by returning a value. Return without expression gives None\n",
        "\n",
        "def area(length, width):\n",
        "  ''' Function calculating the area of a rectangle\n",
        "  Parameters:\n",
        "  length: TYPE float\n",
        "    Measure of the length of a rectangle\n",
        "  width: TYPE float\n",
        "    Measure the width of a rectangle\n",
        "\n",
        "  Return:\n",
        "    area: TYPE float\n",
        "  '''\n",
        "  my_area = length * width\n",
        "\n",
        "  return my_area\n",
        "\n",
        "area(11,4)\n",
        "\n",
        "# Within a function, we can put all kind of instructions, create all kind of objects\n",
        "# Note that the content within the 3 apostrophies is a second way to write some comments.\n",
        "# It is a good practice in order to describe a function."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAy5r4t4NoaG"
      },
      "source": [
        "# To close the chapter on Python basics, there is another way to write a for loop\n",
        "# We will use it in order to delete all the element of a list having a particular value:\n",
        "last_list = [0, 1, 2, 3, 4, 5, 888, 6, 7, 888, 8, 888, 9] # Let's delete all the 888\n",
        "\n",
        "valueToBeRemoved = 888\n",
        "last_list = [value for value in last_list if value != valueToBeRemoved] \n",
        "#of course, you can replace 'value' by what you want, it is the equivalent of i\n",
        "print(last_list)\n",
        "\n",
        "\n",
        "###\n",
        "### End of Python Basics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8n_cd8sNtaq"
      },
      "source": [
        "# End of the Python Basics\n",
        "\n",
        "Now you are have a first overview of what we can do in pure Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-4u_NI7Nzqf"
      },
      "source": [
        "# Start of the Data Manipulation\n",
        "The libraries we will use are pandas and numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NgZ9WJpN3bD"
      },
      "source": [
        "import pandas as pd # Here I import a library named pandas which is specialized in\n",
        "# data manipulation. I give an alias pd it is a kind of pseudonyme where pd refers to pandas.\n",
        "import numpy as np\n",
        "df = pd.read_csv('/content/winequality-red.csv')\n",
        "# Here I use the function read_csv from the pandas package and I assign it to a database named df.\n",
        "# There is a lot of possible parameters in the read_csv function: Always check the documentation.\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
        "\n",
        "# Here df is a database, I will call it as well a dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEJ6pXoeN6Y_"
      },
      "source": [
        "# You can have a look on the first lines:\n",
        "df.head() #You can specify a number in the parenthesis in order to see a specific number of lines\n",
        "df.tail() # It shows the last lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Agvor2SN_1u"
      },
      "source": [
        "# List of useful functions to have general overview on the dataframe:\n",
        "df.dtypes # To see the type of each feature\n",
        "df.describe() # To have some descriptive statistics\n",
        "df.info() # Same as dtypes + more information such as the number of non missing values\n",
        "df.shape # In order to have the number of lines and columns of the dataframe\n",
        "len(df) # In order to have the number of lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mok1o-OoODXB"
      },
      "source": [
        "### Now let's do some data manipulation\n",
        "# Select a column:\n",
        "df['fixed acidity'].head(2) # If you select 1 feature: dataframe['name_feature']\n",
        "# Select multiple columns\n",
        "df[['fixed acidity', 'volatile acidity']].head(2) \n",
        "# If you select 2 or more features: dataframe[['feature1', 'feature2']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIaJLDVWOGHf"
      },
      "source": [
        "# Now, let's do some filters -> select lines from a condition\n",
        "mask1 = df['fixed acidity'] >7 # As we saw in the Python basics part, df['fixed acidity'] >7\n",
        "# refers to a question, does the value (here, each line) is >7 ; True or False\n",
        "# You can display mask1 to see how it is composed, you will see it is a list of boolean.\n",
        "df[mask1].min() # Then, we integreate the list of boulean in: dataframe[list_boolean]\n",
        "# The output is only composed by values >7 that we can check with the .min() function.\n",
        "\n",
        "# You are not obliged to create a mask, you can directly write the condition within:\n",
        "df[df['fixed acidity'] >7]\n",
        "\n",
        "# You can check that indeed the output are exactly the same:\n",
        "df[mask1] == df[df['fixed acidity'] >7]\n",
        "\n",
        "# If you want to select a part of the population between 2 values:\n",
        "mask2 = df['fixed acidity'].between(7,8)\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.Series.between.html\n",
        "\n",
        "df[mask2].min()\n",
        "df[mask2]['fixed acidity'].min()\n",
        "df[mask2][['fixed acidity', 'density']].min()\n",
        "\n",
        "# If you want to do several conditions, the '&' means 'and' ; the '|' means 'or'\n",
        "\n",
        "# Here I want the residual sugars <= 1.9 or >= 2.3 , and sulphates < 0.7301\n",
        "mask3 = (((df['residual sugar'] >= 2.3) | (df['residual sugar'] <= 1.9)) & (df['sulphates'] < 0.7301))\n",
        "df[mask3].describe()\n",
        "\n",
        "# Keep in mind, you have to associate line of script to a variable/object however it\n",
        "# is not saved in memory\n",
        "len(df)\n",
        "df2 = df[mask3]\n",
        "len(df2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM44ToDQOKbJ"
      },
      "source": [
        "# Here I create a function having the name citric_acid_cat with the parameter x\n",
        "# In the function, x will pass within a conditional statement. Given the value,\n",
        "# it will return a letter (a category):\n",
        "def citric_acid_cat(x):\n",
        "  if x < 0.21 :\n",
        "    return 'A'\n",
        "  elif x < 0.3 :\n",
        "    return 'B'\n",
        "  else:\n",
        "    return 'C'\n",
        "\n",
        "# Now we want to create a new feature citric_cat, which transform the numerical value\n",
        "# of citric acid to the category defined above for each line of the feature.\n",
        "# We could create a for loop. However the apply lambda is more efficient from a computing point of view \n",
        "# The x refers to the x of the function which is df['citric acid']\n",
        "df['citric_cat'] = df['citric acid'].apply(lambda x: citric_acid_cat(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK7fZTzZONoB"
      },
      "source": [
        "# When you do a mask, it is not only on numerical features but as well on categoricals:\n",
        "mask5 = (((df['residual sugar'] >= 2.3) | (df['residual sugar'] <= 1.9)) & (df['sulphates'] < 0.7301) & (df['citric_cat'] == 'A'))\n",
        "df[mask5]\n",
        "# On this example, I want to focus only on the category A so it is quick to write.\n",
        "# However it is not always the case and you can filter through a list:\n",
        "\n",
        "list_cat_to_keep = ['A', 'C']\n",
        "mask6 = (((df['residual sugar'] >= 2.3) | (df['residual sugar'] <= 1.9)) & (df['sulphates'] < 0.7301) & (df['citric_cat'].isin(list_cat_to_keep)))\n",
        "df[mask6]\n",
        "# dataframe['feature'].isin(list_specified) creates the list (the pandas Series to be precise) of boolean\n",
        "\n",
        "# To finish, you could want all the element of a feature execpt those that you specify in a list.\n",
        "# Let's say, you want to analyse all the citri_cat that are not in the list (here the cat=B)\n",
        "# You can use: ~dataframe['feature'].isin(list_specified) // The '~' means not isin:\n",
        "mask7 = (((df['residual sugar'] >= 2.3) | (df['residual sugar'] <= 1.9)) & (df['sulphates'] < 0.7301) & (~df['citric_cat'].isin(list_cat_to_keep)))\n",
        "df[mask7] #As you see, you only have the category 'B' of citric_cat."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb31pE4-OQtx"
      },
      "source": [
        "# Now let's complexify the database in order to have additional work to do later! Not important to understand\n",
        "# Educational purpose\n",
        "\n",
        "# Here I create a binary feature who will be used as target\n",
        "# We use the numpy.where function which is equivalent to a if-then-else statement\n",
        "# If df['quality'] >= 7 then df['Y']=1, else df['Y']=0\n",
        "df['Y'] = np.where(df['quality']>= 7, 1, 0)\n",
        "\n",
        "# We create as well some binary features of 'fixed acidity' & 'volatile acidity'\n",
        "df['fixed_acidity_cat'] = np.where(df['fixed acidity']<= 8, 0, 1)\n",
        "df['volatile_acidity_cat'] = np.where(df['volatile acidity']>= 0.4, 0, 1)\n",
        "\n",
        "# Here I create some duplicates rows: I do a random sample of df and then I concat them at the end of df\n",
        "temp = df.sample(n=150, random_state=893717398)\n",
        "df = df.append(temp)\n",
        "del(temp)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Here I create a new feature pH2 from pH where I had some missing values\n",
        "df['pH2'] = np.where(df['pH'].index %15 ==0, np.nan, df['pH'])\n",
        "\n",
        "# Here I create a new random feature 'acidity_other' from 'fixed acidity' and I create some missing values\n",
        "rng = np.random.default_rng(893717398)\n",
        "df = df.join(pd.Series(rng.normal(df['fixed acidity'].median(),1,len(df)), name='acidity_other'))\n",
        "df['acidity_other'] = np.where(df['acidity_other'].index %2 !=0, np.nan, df['pH'])\n",
        "\n",
        "#Here it is a function that will automatically create a feature of random dates between a defined interval\n",
        "def random_datetimes_or_dates(start, end, out_format='datetime', n=10): \n",
        "\n",
        "    '''   \n",
        "    unix timestamp is in ns by default. \n",
        "    I divide the unix time value by 10**9 to make it seconds \n",
        "    (or 24*60*60*10**9 to make it days).\n",
        "    The corresponding unit variable is passed to \n",
        "    the pd.to_datetime function. \n",
        "    Values for the (divide_by, unit) pair to select is defined by \n",
        "    the out_format parameter.\n",
        "    for 1 -> out_format='datetime'\n",
        "    for 2 -> out_format=anything else\n",
        "    '''\n",
        "    (divide_by, unit) = (10**9, 's') if out_format=='datetime' else (24*60*60*10**9, 'D')\n",
        "\n",
        "    start_u = start.value//divide_by\n",
        "    end_u = end.value//divide_by\n",
        "\n",
        "    return pd.to_datetime(np.random.randint(start_u, end_u, n), unit=unit)\n",
        "\n",
        "# Now I apply my function in order to create the new feature 'date'\n",
        "np.random.seed(893717398)\n",
        "d_start = pd.to_datetime('2021-01-01')\n",
        "d_end = pd.to_datetime('2021-06-30')\n",
        "df['date'] = random_datetimes_or_dates(d_start, d_end, out_format='datetime', n=len(df))\n",
        "df['date'] = df['date'].astype(str)\n",
        "\n",
        "## End of the complexification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEoYXNjNOWzO"
      },
      "source": [
        "# Have a look on the etadata of the dataframe:\n",
        "df.shape\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9SdBGXCOYyg"
      },
      "source": [
        "# Have a look on the describe() function applied to a categorical feature:\n",
        "df['citric_cat'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hJe4epeObO2"
      },
      "source": [
        "# Let's say that you want to replace the category 'C' by the category 'Z:\n",
        "df['citric_cat'] = np.where(df['citric_cat'] == 'C', 'Z', df['citric_cat'])\n",
        "# You could use as well the replace() function from pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMAE4QT4Od4C"
      },
      "source": [
        "# One of the most useful function in order to do some counts: value_counts()\n",
        "df['citric_cat'].value_counts()\n",
        "df['citric_cat'].value_counts(True)\n",
        "np.round(df['citric_cat'].value_counts(True)*100, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cRYwZYMOgwy"
      },
      "source": [
        "# Let's see if we have some missing values (NaN for Not a Number) with the function isna():\n",
        "df.isna()\n",
        "df.isna().sum() # Sum per column\n",
        "#df.isna().sum(1) # Sum per rows\n",
        "# The treatment of missing values for a modeling purpose will be in another session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlripKZcOjge"
      },
      "source": [
        "# Let's parsing some date.\n",
        "# Parsing a date means to make a date understandable by a computer. Without parsing, it is \n",
        "# considered as a chain of character.\n",
        "df['date'].head(3)\n",
        "df['date'] = pd.to_datetime(df['date'], format= '%Y-%m-%d %H:%M:%S') # -> ALWAYS specify the format! <-\n",
        "# For all the existing formats:\n",
        "# https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
        "\n",
        "# Now we can work with the date feature in order to create a duration\n",
        "today = pd.to_datetime('2021-10-12', format = '%Y-%m-%d')\n",
        "today\n",
        "df['time_diff'] = today - df['date']\n",
        "df['time_diff'].head(3)\n",
        "df.dtypes\n",
        "# We can as well transform the timedelta in a numerical form:\n",
        "df['day_diff'] = (df['time_diff']/np.timedelta64(1, 'D')).astype(float)\n",
        "df['day_diff']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjqe8Yd1OmgG"
      },
      "source": [
        "# Now let's pratice some merges\n",
        "# First, let's create a new database randomf1 & randomf2:\n",
        "np.random.seed(893717398)\n",
        "randomf1 = pd.Series(np.random.normal(0, 1, len(df)), name='Random_Normal_feature1')\n",
        "randomf2 = pd.Series(np.random.normal(1, 0.5, len(df)), name='Random_Normal_feature2')\n",
        "\n",
        "# First way to do a merge: Doing a concatenation\n",
        "# Be careful because it has some limitation!!!!\n",
        "# You need to be sure at 100% that each rows of the 2 database to concat are referring to the same observation!!\n",
        "randomf = pd.concat([randomf1, randomf2], axis=1, ignore_index=True)\n",
        "randomf.columns = ['Random_Normal_feature1', 'Random_Normal_feature2']\n",
        "\n",
        "# Here I create a a new feature in df that will be used as key:\n",
        "np.random.seed(893717398)\n",
        "df['Random_Normal_feature1'] = pd.Series(np.random.normal(0, 1, len(df)), name='Random_Normal_feature1')\n",
        "\n",
        "# Second way to do a merge: Using the merge function:\n",
        "df = df.merge(randomf, how='inner', on='Random_Normal_feature1')\n",
        "# Have a look on the documentation to see all the possibilities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZWmLnIsOpsi"
      },
      "source": [
        "# We can easily do some agregations of features through a groupby:\n",
        "df.groupby('citric_cat')['quality'].mean()\n",
        "df.groupby(['citric_cat', 'quality'])[['sulphates', 'fixed acidity']].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MedGJCgjOtAp"
      },
      "source": [
        "# Last element very useful in data manipulation is the pivot table:\n",
        "pd.pivot_table(df, values='sulphates', index=['Y', 'quality'],\n",
        "                    columns=['citric_cat'], aggfunc=np.mean)\n",
        "# Go to see the documentation for additional details"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8PkRnPKOwqx"
      },
      "source": [
        "**End of the data manipulation part with pandas and numpy !**\n",
        "\n",
        "Now you have the basis to manipulate every databases and to agregate or to obtain summary statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSZqotxhO2BZ"
      },
      "source": [
        "# Start of the Data Visualization\n",
        "The libraries we will use are matplotlib and seaborn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WKYFfuQO7po"
      },
      "source": [
        "# Let's import new packages:\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "from matplotlib.lines import Line2D\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDMqTm6T6_MX",
        "outputId": "60d1b6eb-6c00-45ea-eab6-75b088ec880e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "# Bar plot\n",
        "# Let's represent visually the median wine quality by citric_cat\n",
        "# First: represent the database:\n",
        "df_barplot = pd.DataFrame(df.groupby('citric_cat')['quality'].median()).reset_index() # Citric_cat\n",
        "# is an ordered feature, so we will not change the order in the plot\n",
        "df_barplot"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5aa0fdc3a1d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Let's represent visually the median wine quality by citric_cat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# First: represent the database:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_barplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'citric_cat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quality'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Citric_cat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# is an ordered feature, so we will not change the order in the plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_barplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T5xkqXo7CHF"
      },
      "source": [
        "# We need to define a figure and a Axes:\n",
        "fig1, ax = plt.subplots() # It always starts like this\n",
        "# Define the kind of visualization you want, here it's a bar plot\n",
        "ax.bar(x= df_barplot['citric_cat'], height=df_barplot['quality'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOSvZlqy7Xi9"
      },
      "source": [
        "# Let's add a horizontal line showing the median -0.5 quality off all wines\n",
        "med_quality = df['quality'].median() - 0.5\n",
        "\n",
        "fig1, ax = plt.subplots()\n",
        "ax.bar(x= df_barplot['citric_cat'], height=df_barplot['quality'])\n",
        "ax.axhline(y=med_quality, color='#058ED9')\n",
        "ax.set_title('Median quality by citric_cat', loc='center', fontsize=16)\n",
        "ax.set_ylabel('Median quality', fontsize = 15)\n",
        "ax.set_xlabel('citric_cat', fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7yYObzj7asu"
      },
      "source": [
        "# Let's make the plot pretty:\n",
        "med_quality = df['quality'].median() - 0.5\n",
        "\n",
        "fig1, ax = plt.subplots()\n",
        "ax.bar(x= df_barplot['citric_cat'], height=df_barplot['quality'])\n",
        "ax.axhline(y=med_quality, color='#058ED9')\n",
        "\n",
        "ax.set_title('Median quality by citric_cat', loc='center', fontsize=16)\n",
        "ax.set_ylabel('Median quality', fontsize = 15)\n",
        "ax.set_xlabel('citric_cat', fontsize=15)\n",
        "\n",
        "# Add a grid composed only by the y axis\n",
        "ax.grid(b=True, which='major', axis='y')\n",
        "\n",
        "# Delete useless visual lines:\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['bottom'].set_visible(False)\n",
        "ax.spines['left'].set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCiCpqkg7gdg"
      },
      "source": [
        "df_lineplot = df[['date', 'quality', 'residual sugar']].sort_values(by='date')\n",
        "df_lineplot=df_lineplot.set_index('date').resample('W').mean().reset_index()\n",
        "df_lineplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z39RiafU7wK1"
      },
      "source": [
        "# Line plot: let's represent the evolution of the average quality and the average residual\n",
        "# sugar per week:\n",
        "\n",
        "df_lineplot = df[['date', 'quality', 'residual sugar']].sort_values(by='date')\n",
        "df_lineplot=df_lineplot.set_index('date').resample('W').mean().reset_index()\n",
        "\n",
        "fig2, ax = plt.subplots()\n",
        "ax.plot(df_lineplot.date, df_lineplot['quality'], color='#6FDE6E', label='quality')\n",
        "ax.plot(df_lineplot.date, df_lineplot['residual sugar'], color='#FF4242', label='residual sugar')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-ug7R5p76st"
      },
      "source": [
        "# With a legend\n",
        "\n",
        "fig2, ax = plt.subplots()\n",
        "ax.plot(df_lineplot.date, df_lineplot['quality'], color='#6FDE6E', label='quality')\n",
        "ax.plot(df_lineplot.date, df_lineplot['residual sugar'], color='#FF4242', label='residual sugar')\n",
        "\n",
        "# Add a legend:\n",
        "ax.legend(loc='center left', bbox_to_anchor=(1.04,0.5), ncol=1, borderaxespad=0, frameon=False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FkBum997_T-"
      },
      "source": [
        "# Make it pretty\n",
        "\n",
        "fig2, ax = plt.subplots()\n",
        "ax.plot(df_lineplot.date, df_lineplot['quality'], color='#6FDE6E', label='quality')\n",
        "ax.plot(df_lineplot.date, df_lineplot['residual sugar'], color='#FF4242', label='residual sugar')\n",
        "\n",
        "ax.legend(loc='center left', bbox_to_anchor=(1.04,0.5), ncol=1, borderaxespad=0, frameon=False)\n",
        "ax.set_title('Weekly evolution of average \\nquality & residual sugar', loc='center', fontsize=16)\n",
        "ax.set_ylabel('Average evolution', fontsize=15)\n",
        "\n",
        "ax.grid(b=True, which='major', axis='y')\n",
        "ax.axhline(3, color='black')\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['bottom'].set_visible(False)\n",
        "ax.spines['left'].set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZOY5pr08DcY"
      },
      "source": [
        "# Change date display\n",
        "months=mdates.MonthLocator()\n",
        "x_display = mdates.DateFormatter('%d-%b')\n",
        "\n",
        "fig2, ax = plt.subplots()\n",
        "ax.plot(df_lineplot.date, df_lineplot['quality'], color='#6FDE6E', label='quality')\n",
        "ax.plot(df_lineplot.date, df_lineplot['residual sugar'], color='#FF4242', label='residual sugar')\n",
        "\n",
        "ax.legend(loc='center left', bbox_to_anchor=(1.04,0.5), ncol=1, borderaxespad=0, frameon=False)\n",
        "ax.set_title('Weekly evolution of average \\nquality & residual sugar', loc='center', fontsize=16)\n",
        "ax.set_ylabel('Average evolution', fontsize=15)\n",
        "\n",
        "ax.xaxis.set_major_locator(months)\n",
        "ax.xaxis.set_major_formatter(x_display)\n",
        "\n",
        "ax.grid(b=True, which='major', axis='y')\n",
        "ax.axhline(3, color='black')\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['bottom'].set_visible(False)\n",
        "ax.spines['left'].set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyU6MKT88HE1"
      },
      "source": [
        "# Now let's try to put to plots on the same figure\n",
        "# You will see the utility of Axes\n",
        "med_quality = df['quality'].median() - 0.5\n",
        "months=mdates.MonthLocator()\n",
        "x_display = mdates.DateFormatter('%d-%b')\n",
        "\n",
        "fig3 = plt.figure(constrained_layout=True, figsize=(15,10))\n",
        "gs = fig3.add_gridspec(2,2)\n",
        "ax1 = fig3.add_subplot(gs[0,0])\n",
        "ax2 = fig3.add_subplot(gs[0,1])\n",
        "\n",
        "ax1.bar(x= df_barplot['citric_cat'], height=df_barplot['quality'])\n",
        "ax1.axhline(y=med_quality, color='#058ED9')\n",
        "\n",
        "ax1.set_title('Median quality by citric_cat', loc='center', fontsize=16)\n",
        "ax1.set_ylabel('Median quality', fontsize = 15)\n",
        "ax1.set_xlabel('citric_cat', fontsize=15)\n",
        "\n",
        "# Add a grid composed only by the y axis\n",
        "ax1.grid(b=True, which='major', axis='y')\n",
        "\n",
        "# Delete useless visual lines:\n",
        "ax1.spines['top'].set_visible(False)\n",
        "ax1.spines['right'].set_visible(False)\n",
        "ax1.spines['bottom'].set_visible(False)\n",
        "ax1.spines['left'].set_visible(False)\n",
        "\n",
        "\n",
        "ax2.plot(df_lineplot.date, df_lineplot['quality'], color='#6FDE6E', label='quality')\n",
        "ax2.plot(df_lineplot.date, df_lineplot['residual sugar'], color='#FF4242', label='residual sugar')\n",
        "\n",
        "ax2.legend(loc='center left', bbox_to_anchor=(1.04,0.5), ncol=1, borderaxespad=0, frameon=False)\n",
        "ax2.set_title('Weekly evolution of average \\nquality & residual sugar', loc='center', fontsize=16)\n",
        "ax2.set_ylabel('Average evolution', fontsize=15)\n",
        "\n",
        "ax2.xaxis.set_major_locator(months)\n",
        "ax2.xaxis.set_major_formatter(x_display)\n",
        "\n",
        "ax2.grid(b=True, which='major', axis='y')\n",
        "ax2.axhline(3, color='black')\n",
        "ax2.spines['top'].set_visible(False)\n",
        "ax2.spines['right'].set_visible(False)\n",
        "ax2.spines['bottom'].set_visible(False)\n",
        "ax2.spines['left'].set_visible(False)\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGOiwmFZ8LW9"
      },
      "source": [
        "# Bonus example\n",
        "# You can use as well seaborn who have built-in plots\n",
        "# The syntax is simplier and this is a layer of matplotlib\n",
        "# which means that you can insert seaborn code in a matplotlib figure!!\n",
        "\n",
        "fig4, ax = plt.subplots()\n",
        "ax = sns.scatterplot(data=df, x=\"pH\", y=\"fixed acidity\")\n",
        "\n",
        "ax.set_title('Scatter plot between fixed acidity and pH', loc='center', fontsize=16)\n",
        "\n",
        "ax.grid(b=True, which='major', axis='both')\n",
        "\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['bottom'].set_visible(False)\n",
        "ax.spines['left'].set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmySYnyv9Akz"
      },
      "source": [
        "# End of the Data Visualization with matplotlib & seaborn!\n",
        "\n",
        "Now you are able to do any kind of Data Visualization with matplotlib and seaborn!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-HdAFxF9DRs"
      },
      "source": [
        "# Data Cleaning for modeling\n",
        "In this chapter we will see the differents steps to have in mind in order to allow any kind of algorithm to catch the information from the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohEZIzrV8PL3"
      },
      "source": [
        "# Here is a sumup of all the manipulation we did in order to load the dataset \n",
        "# and to add some complexity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "from matplotlib.lines import Line2D\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import scipy.stats as ss\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "def citric_acid_cat(x):\n",
        "  if x < 0.21 :\n",
        "    return 'A'\n",
        "  elif x < 0.3 :\n",
        "    return 'B'\n",
        "  else:\n",
        "    return 'C'\n",
        "\n",
        "df = pd.read_csv('/content/winequality-red.csv')\n",
        "df['citric_cat'] = df['citric acid'].apply(lambda x: citric_acid_cat(x))\n",
        "df['Y'] = np.where(df['quality']>= 7, 1, 0)\n",
        "df['fixed_acidity_cat'] = np.where(df['fixed acidity']<= 8, 0, 1)\n",
        "df['volatile_acidity_cat'] = np.where(df['volatile acidity']>= 0.4, 0, 1)\n",
        "temp = df.sample(n=150, random_state=893717398)\n",
        "df = df.append(temp)\n",
        "del(temp)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df['pH2'] = np.where(df['pH'].index %15 ==0, np.nan, df['pH'])\n",
        "rng = np.random.default_rng(893717398)\n",
        "df = df.join(pd.Series(rng.normal(df['fixed acidity'].median(),1,len(df)), name='acidity_other'))\n",
        "df['acidity_other'] = np.where(df['acidity_other'].index %2 !=0, np.nan, df['pH'])\n",
        "def random_datetimes_or_dates(start, end, out_format='datetime', n=10): \n",
        "\n",
        "    '''   \n",
        "    unix timestamp is in ns by default. \n",
        "    I divide the unix time value by 10**9 to make it seconds \n",
        "    (or 24*60*60*10**9 to make it days).\n",
        "    The corresponding unit variable is passed to \n",
        "    the pd.to_datetime function. \n",
        "    Values for the (divide_by, unit) pair to select is defined by \n",
        "    the out_format parameter.\n",
        "    for 1 -> out_format='datetime'\n",
        "    for 2 -> out_format=anything else\n",
        "    '''\n",
        "    (divide_by, unit) = (10**9, 's') if out_format=='datetime' else (24*60*60*10**9, 'D')\n",
        "\n",
        "    start_u = start.value//divide_by\n",
        "    end_u = end.value//divide_by\n",
        "\n",
        "    return pd.to_datetime(np.random.randint(start_u, end_u, n), unit=unit)\n",
        "np.random.seed(893717398)\n",
        "d_start = pd.to_datetime('2021-01-01')\n",
        "d_end = pd.to_datetime('2021-06-30')\n",
        "df['date'] = random_datetimes_or_dates(d_start, d_end, out_format='datetime', n=len(df))\n",
        "df['date'] = df['date'].astype(str)\n",
        "df['citric_cat'] = np.where(df['citric_cat'] == 'C', 'Z', df['citric_cat'])\n",
        "df['date'] = pd.to_datetime(df['date'], format= '%Y-%m-%d %H:%M:%S') \n",
        "today = pd.to_datetime('2021-10-12', format = '%Y-%m-%d')\n",
        "df['time_diff'] = today - df['date']\n",
        "df['day_diff'] = (df['time_diff']/np.timedelta64(1, 'D')).astype(float)\n",
        "np.random.seed(893717398)\n",
        "randomf1 = pd.Series(np.random.normal(0, 1, len(df)), name='Random_Normal_feature1')\n",
        "randomf2 = pd.Series(np.random.normal(1, 0.5, len(df)), name='Random_Normal_feature2')\n",
        "randomf = pd.concat([randomf1, randomf2], axis=1, ignore_index=True)\n",
        "randomf.columns = ['Random_Normal_feature1', 'Random_Normal_feature2']\n",
        "np.random.seed(893717398)\n",
        "df['Random_Normal_feature1'] = pd.Series(np.random.normal(0, 1, len(df)), name='Random_Normal_feature1')\n",
        "df = df.merge(randomf, how='inner', on='Random_Normal_feature1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9BKi0Ve_XSC"
      },
      "source": [
        "# First of all, we need to split the data base into 3 parts:\n",
        "# - One for the training part\n",
        "# - One for the validation part\n",
        "# - One for the temporal validation part (OOT: Out Of Time)\n",
        "\n",
        "# Let's do the first temporal split:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzRcq5zi_iwv"
      },
      "source": [
        "# We already parsed our date feature previously (you can check with dtypes)\n",
        "# Let's see the monthly number of observation\n",
        "df.set_index('date').resample('M').size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN9iYmIH_yr3"
      },
      "source": [
        "# We will use the 2 last months for the OOT validation\n",
        "oot = df[df['date'] >= '2021-05-01']\n",
        "# You can check if it works as you want: oot.date.min()\n",
        "\n",
        "# Now we will keep the database without the oot:\n",
        "df = df[df['date'] < '2021-05-01']\n",
        "# And we will randomly split this data base in 2, one for the train and the other for the test\n",
        "# As you notice we will have to validation database.\n",
        "# One based on the same distribution as the train and the other from another distribution\n",
        "\n",
        "# The random split is used with the sklean library, I decided to have 75% in train and 25% in test\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df, test_size=0.25, random_state=893717398)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDALmp90AGPZ"
      },
      "source": [
        "# Let's see if we have duplicated rows:\n",
        "feature_analyze_duplicates = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
        "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
        "       'pH', 'sulphates', 'alcohol', 'quality', 'citric_cat',\n",
        "       'fixed_acidity_cat', 'volatile_acidity_cat', 'pH2', 'acidity_other']\n",
        "\n",
        "len(train[train.duplicated(subset = feature_analyze_duplicates, keep=False)])\n",
        "# We see that we have 104 rows that are duplicated, we can decide to keep the first or last.\n",
        "# Let's create a function that will automatize this process\n",
        "\n",
        "def drop_duplicate_keep_first(database, list_of_features):\n",
        "  database = database.drop_duplicates(subset= list_of_features, keep='first')\n",
        "  return database\n",
        "\n",
        "train = drop_duplicate_keep_first(train, feature_analyze_duplicates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N2aWa6xAMZY"
      },
      "source": [
        "# Let's see the features distributions in order to see if we have strange / extrem values\n",
        "continuous_feature_analyze_distribution = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
        "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
        "       'pH', 'sulphates', 'alcohol', 'quality',\n",
        "       'fixed_acidity_cat', 'volatile_acidity_cat', 'pH2', 'acidity_other', 'day_diff',\n",
        "       'Random_Normal_feature1', 'Random_Normal_feature2']\n",
        "\n",
        "\n",
        "for i, col in enumerate(train[continuous_feature_analyze_distribution]):\n",
        "  plt.figure(i)\n",
        "  sns.kdeplot(x=col, data=train)\n",
        "  \n",
        "# fixed acidity has no strange / extrem values\n",
        "# volatile acidity has no strange / extrem values\n",
        "# citric acidity has no strange / extrem values | The min=0 be careful with the plot\n",
        "# residual sugar has extrem values -> we can do a censure at 10\n",
        "# chlorides has extrem values -> we can do a censure at 0.3\n",
        "# free sulfur dioxide has extrem values -> we can do a censure at 60\n",
        "# total sulfur dioxide has extrem values -> we can do a censure at 200\n",
        "# sulphates has extrem values -> we can do a censure at 1.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGie4jxWARWQ"
      },
      "source": [
        "def postitive_censure(database, feature_to_censure, value_censure):\n",
        "  feature = np.where(database[feature_to_censure] >= value_censure, value_censure, database[feature_to_censure])\n",
        "  return feature\n",
        "\n",
        "train['residual sugar'] = postitive_censure(train, 'residual sugar', 10)\n",
        "train['chlorides'] = postitive_censure(train, 'chlorides', 0.3)\n",
        "train['free sulfur dioxide'] = postitive_censure(train, 'free sulfur dioxide', 60)\n",
        "train['total sulfur dioxide'] = postitive_censure(train, 'total sulfur dioxide', 200)\n",
        "train['sulphates'] = postitive_censure(train, 'sulphates', 1.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnphGy7SAVNJ"
      },
      "source": [
        "# We know that we have missing values, let's treat them\n",
        "train.isna().sum() # They are for pH2 & acidity_other\n",
        "\n",
        "# There are several ways to treat missing values for numerical features:\n",
        "# Imputation by the median (or other numerical form) or by an impossible value\n",
        "# Imputation by the median (or other numerical form) and create a binary feature\n",
        "\n",
        "# For a categorical feature:\n",
        "# - You can impute by a category named 'Missing'\n",
        "# - Sometimes, a missing value has a real world meaning so you can impute it by what it means\n",
        "\n",
        "# Let's do the second method.\n",
        "# I suppose a correlation between 'fixed acidity' and pH2 of -0.69\n",
        "\n",
        "# First, let's create a dummy feature if the value is missing\n",
        "# train['pH2_missing'] = np.where(train['pH2'].isna(), 1, 0)\n",
        "# Transform it in a function:\n",
        "def create_dummy_if_missing(database, feature_with_missings):\n",
        "  feature = np.where(database[feature_with_missings].isna(), 1, 0)\n",
        "  return feature\n",
        "\n",
        "train['pH2_missing'] = create_dummy_if_missing(train, 'pH2')\n",
        "train['acidity_other_missing'] = create_dummy_if_missing(train, 'acidity_other')\n",
        "\n",
        "# This line will impute for all the train, let's change it to a function\n",
        "# train.loc[train['pH2'].isna(), \"pH2\"] = train.loc[train['pH2'].isna(), \"fixed acidity\"]*-0.69\n",
        "\n",
        "def impute_multiply_value(database, feature_to_impute, feature_reference, coef):\n",
        "  database.loc[database[feature_to_impute].isna(), feature_to_impute] = database.loc[database[feature_to_impute].isna(), feature_reference]*coef\n",
        "\n",
        "impute_multiply_value(train, 'pH2', 'fixed acidity', -0.69)\n",
        "# Note that I used fixed acidity, but a deeper correlation analysis would help you to find what suits the best\n",
        "\n",
        "# For acidity_other, we will impute by the median\n",
        "median_acidity_other = train['acidity_other'].median()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2xJ9MB0Ac-v"
      },
      "source": [
        "def impute_missing_by_median(database, feature_with_missings, value_to_impute):\n",
        "  feature = np.where(database[feature_with_missings].isna(), value_to_impute, database[feature_with_missings])\n",
        "  return feature\n",
        "\n",
        "train['acidity_other'] = impute_missing_by_median(train, 'acidity_other', median_acidity_other)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGEMIhJoAlZZ"
      },
      "source": [
        "# Now let's check if the 2 distributions changed between before and after the imputation\n",
        "continuous_feature_analyze_distribution_after_imputation = ['pH2', 'acidity_other']\n",
        "\n",
        "for i, col in enumerate(train[continuous_feature_analyze_distribution_after_imputation]):\n",
        "  plt.figure(i)\n",
        "  sns.kdeplot(x=col, data=train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjzaHf0IAtDG"
      },
      "source": [
        "# pH2 shoud be between 2.8 & 4\n",
        "train['pH2'] = postitive_censure(train, 'pH2', 4)\n",
        "\n",
        "def negative_censure(database, feature_to_censure, value_censure):\n",
        "  feature = np.where(database[feature_to_censure] <= value_censure, value_censure, database[feature_to_censure])\n",
        "  return feature\n",
        "\n",
        "train['pH2'] = negative_censure(train, 'pH2', 2.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDrsp0z-CnKe"
      },
      "source": [
        "sns.kdeplot(x='pH2', data=train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1ZuNSCbCq9q"
      },
      "source": [
        "# Now let's analyze the correlations between the quantitative features\n",
        "# Keep in mind that binary features are not quantitative !!!\n",
        "feature_analyze_correlation = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
        "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
        "       'pH', 'sulphates', 'alcohol', 'pH2', 'acidity_other',\n",
        "       'day_diff', 'Random_Normal_feature1',\n",
        "       'Random_Normal_feature2']\n",
        "\n",
        "# Get all the correlations:\n",
        "corr = train[feature_analyze_correlation].corr()\n",
        "#\n",
        "# Set up the matplotlib plot configuration\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "# Generate a mask for upper traingle\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "# Configure a custom diverging colormap\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "# Draw the heatmap\n",
        "sns.heatmap(corr, annot=True, mask = mask, cmap=cmap)\n",
        "\n",
        "# We can see on the heatmap that we don't have a features having a high correlation (>0.8)\n",
        "# As a result we can keep them all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePraYLIcCu6V"
      },
      "source": [
        "def cramers_corrected_stat(confusion_matrix):\n",
        "    \"\"\" calculate Cramers V statistic for categorical-categorical association.\n",
        "        uses correction from Bergsma and Wicher, \n",
        "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
        "    \"\"\"\n",
        "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    phi2 = chi2/n\n",
        "    r,k = confusion_matrix.shape\n",
        "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n",
        "    rcorr = r - ((r-1)**2)/(n-1)\n",
        "    kcorr = k - ((k-1)**2)/(n-1)\n",
        "    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n",
        "\n",
        "\n",
        "cat_feature_analyze_correlation = ['citric_cat', 'fixed_acidity_cat', 'volatile_acidity_cat', 'pH2_missing', 'acidity_other_missing']\n",
        "corrM = np.zeros((len(cat_feature_analyze_correlation),len(cat_feature_analyze_correlation)))\n",
        "# there's probably a nice pandas way to do this\n",
        "for col1, col2 in itertools.combinations(cat_feature_analyze_correlation, 2):\n",
        "    idx1, idx2 = cat_feature_analyze_correlation.index(col1), cat_feature_analyze_correlation.index(col2)\n",
        "    corrM[idx1, idx2] = cramers_corrected_stat(pd.crosstab(train[col1], train[col2]))\n",
        "    corrM[idx2, idx1] = corrM[idx1, idx2]\n",
        "\n",
        "corr_cramV = pd.DataFrame(corrM, index=cat_feature_analyze_correlation, columns=cat_feature_analyze_correlation)\n",
        "mask = np.triu(np.ones_like(corr_cramV, dtype=bool))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "ax = sns.heatmap(corr_cramV, annot=True, ax=ax, mask=mask)\n",
        "ax.set_title(\"Cramer V Correlation between Variables\")\n",
        "plt.show()\n",
        "\n",
        "# Same here, we don't have highly correletaed features so we can keep them."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39DGQHDrC0dQ"
      },
      "source": [
        "# Now we can do some feature engineering in order to build new features based on some others.\n",
        "# This step is only based on domain knowledge & creativity.\n",
        "# Here I will just create 3 features\n",
        "\n",
        "def acidity_total(database):\n",
        "  feature = database['fixed acidity'] + database['volatile acidity']\n",
        "  return feature\n",
        "\n",
        "def ratio_density_alcohol(database):\n",
        "  feature = database['density'] / database['alcohol']\n",
        "  return feature\n",
        "\n",
        "def ratio_acidity_pH(database):\n",
        "  feature = database['acidity_tot'] / database['pH']\n",
        "  return feature\n",
        "\n",
        "train['acidity_tot'] = acidity_total(train)\n",
        "train['ratio_density_alcohol'] = ratio_density_alcohol(train)\n",
        "train['ratio_acidity_pH'] = ratio_acidity_pH(train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQIIZ56rC4Jt"
      },
      "source": [
        "# Now we have to manage the categorical features in order to transform them in numerical\n",
        "# An algorithm is just a mathematical process. As a result, it deals only with numerical values\n",
        "\n",
        "# Most common: One Hot Encoding & used\n",
        "# Most smart: Target Encoding\n",
        "# Not to use (except in some very particular cases): Label Encoding\n",
        "\n",
        "def one_hot_encoding(database, columns_to_OHE):\n",
        "  database = pd.get_dummies(database, columns = columns_to_OHE)\n",
        "  return database\n",
        "\n",
        "train = one_hot_encoding(train, ['citric_cat'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOpBul9nC7bw"
      },
      "source": [
        "train.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90xtnv2eC-RB"
      },
      "source": [
        "# Let's define the feature we will use for X & Y\n",
        "X_features = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
        "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
        "       'pH', 'sulphates', 'alcohol', 'fixed_acidity_cat',\n",
        "       'volatile_acidity_cat', 'pH2', 'acidity_other',\n",
        "       'day_diff', 'Random_Normal_feature1', 'Random_Normal_feature2',\n",
        "       'pH2_missing', 'acidity_other_missing', 'acidity_tot',\n",
        "       'ratio_density_alcohol', 'ratio_acidity_pH', 'citric_cat_A',\n",
        "       'citric_cat_B', 'citric_cat_Z']\n",
        "\n",
        "Y_feature = ['Y']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N91qUBPjDBNu"
      },
      "source": [
        "print('Number of observations=1: ' + str(train.Y.sum()))\n",
        "print('\\nPercentage of observations=1: ' + str(train.Y.mean()*100))\n",
        "\n",
        "# Based on this percentage we need to rebalance our data set in order to increase the percentage at least until 30%\n",
        "# Easiest method: oversampling having the objective to add (to copy) the positive labels\n",
        "# Second method is undersampling having the objective to delete some negative labels\n",
        "# Best method based on machine learning: SMOTE -> A little bit better than the previous one but more complex. \n",
        "# Let's try the smote for this session\n",
        "\n",
        "sm = SMOTE(random_state=893717398, sampling_strategy=0.55)\n",
        "xTrain, yTrain = sm.fit_resample(train[X_features], train[Y_feature])\n",
        "\n",
        "# Now let's transform the data in array to a dataframe\n",
        "train_smote = pd.concat([xTrain, yTrain], axis=1)\n",
        "train_smote = train_smote.sample(n=len(train_smote), random_state=893717398)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-u4nujTDE4N"
      },
      "source": [
        "# -> Important: At this step you can still have a lot of features. It is necessary to select the best one\n",
        "# We will try it even if 26 features is not that much."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHeV5xa-DKLP"
      },
      "source": [
        "# We will do this method 2 times, a first one in order to test a linear modeling.\n",
        "# A second one in order to test a non-linear modeling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQnNEKBnDMm-"
      },
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_feat_selec = RandomForestClassifier(bootstrap=True, # resampling of the database for each tree\n",
        "                                       class_weight=None, #we already resampled before with SMOTE\n",
        "                                       criterion='gini', #gini or entropy, not very important\n",
        "                                       max_depth=5, #take a number between 5&8 depending of the size of the database\n",
        "                                       max_features='auto', #number of featuer selected by tree\n",
        "                                       max_leaf_nodes=None, #not important to change\n",
        "                                       min_samples_leaf=30, #Because we have a small database, 150 is good\n",
        "                                       n_estimators=500, #Number of trees\n",
        "                                       n_jobs=-1, #We take all the processors -1 that we have\n",
        "                                       random_state=893717398)\n",
        "\n",
        "rfecv_feat_selec = RFECV(estimator=rf_feat_selec,\n",
        "                         step=1,\n",
        "                         cv=3,\n",
        "                         scoring='roc_auc',\n",
        "                         n_jobs=-1)\n",
        "\n",
        "rfecv_feat_selec.fit(xTrain, yTrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT6YVJiMDQgO"
      },
      "source": [
        "print(rfecv_feat_selec.n_features_)\n",
        "print(rfecv_feat_selec.support_)\n",
        "\n",
        "plt.figure()\n",
        "plt.xlabel('Nbr features selected')\n",
        "plt.ylabel('Cross val')\n",
        "plt.plot(range(1, len(rfecv_feat_selec.grid_scores_)+1), rfecv_feat_selec.grid_scores_)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS-YvFF_DVr4"
      },
      "source": [
        "n_features=10\n",
        "\n",
        "rfe_feat_selec = RFE(estimator=rf_feat_selec, \n",
        "                     step=1,\n",
        "                     n_features_to_select=n_features)\n",
        "rfe_feat_selec.fit(xTrain, yTrain)\n",
        "\n",
        "selected_X = pd.DataFrame(xTrain, columns=X_features).columns[rfe_feat_selec.get_support(indices=True)]\n",
        "\n",
        "del(xTrain, yTrain) # I delete them in order to make easy by keeping a dataframe instead of an array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9VbCvyBDdLv"
      },
      "source": [
        "# Get the final list of features and add the target\n",
        "final_list_features = selected_X.append(pd.Index(['Y']))\n",
        "final_list_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWikp0pXDghO"
      },
      "source": [
        "# Now apply all of the cleaning work we did on the test and temporal dataset."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfEzXNt7Dq01"
      },
      "source": [
        "# Let's apply all the treatments that we did on the test & oot datasets.\n",
        "# You can see the reason why we created a lot of functions -> it is much more\n",
        "# efficient to use.\n",
        "\n",
        "# I put a lot of comments because unfortunatly several treatments are not useful\n",
        "# for the final features selected. As a result, it is useless to make it run by the server.\n",
        "\n",
        "test = drop_duplicate_keep_first(test, feature_analyze_duplicates)\n",
        "oot = drop_duplicate_keep_first(oot, feature_analyze_duplicates)\n",
        "\n",
        "#test['residual sugar'] = postitive_censure(test, 'residual sugar', 10)\n",
        "#oot['residual sugar'] = postitive_censure(oot, 'residual sugar', 10)\n",
        "\n",
        "test['chlorides'] = postitive_censure(test, 'chlorides', 0.3)\n",
        "oot['chlorides'] = postitive_censure(oot, 'chlorides', 0.3)\n",
        "\n",
        "#test['free sulfur dioxide'] = postitive_censure(test, 'free sulfur dioxide', 60)\n",
        "#oot['free sulfur dioxide'] = postitive_censure(oot, 'free sulfur dioxide', 60)\n",
        "\n",
        "test['total sulfur dioxide'] = postitive_censure(test, 'total sulfur dioxide', 200)\n",
        "oot['total sulfur dioxide'] = postitive_censure(oot, 'total sulfur dioxide', 200)\n",
        "\n",
        "test['sulphates'] = postitive_censure(test, 'sulphates', 1.5)\n",
        "oot['sulphates'] = postitive_censure(oot, 'sulphates', 1.5)\n",
        "\n",
        "#test['pH2_missing'] = create_dummy_if_missing(test, 'pH2')\n",
        "#oot['pH2_missing'] = create_dummy_if_missing(oot, 'pH2')\n",
        "\n",
        "#test['acidity_other_missing'] = create_dummy_if_missing(test, 'acidity_other')\n",
        "#oot['acidity_other_missing'] = create_dummy_if_missing(oot, 'acidity_other')\n",
        "\n",
        "#impute_multiply_value(test, 'pH2', 'fixed acidity', -0.69)\n",
        "#impute_multiply_value(oot, 'pH2', 'fixed acidity', -0.69)\n",
        "\n",
        "#test['acidity_other'] = impute_missing_by_median(test, 'acidity_other', median_acidity_other)\n",
        "#oot['acidity_other'] = impute_missing_by_median(oot, 'acidity_other', median_acidity_other)\n",
        "\n",
        "#test['pH2'] = postitive_censure(test, 'pH2', 4)\n",
        "#oot['pH2'] = postitive_censure(oot, 'pH2', 4)\n",
        "\n",
        "#test['pH2'] = negative_censure(test, 'pH2', 2.8)\n",
        "#oot['pH2'] = negative_censure(oot, 'pH2', 2.8)\n",
        "\n",
        "#test['acidity_tot'] = acidity_total(test)\n",
        "#oot['acidity_tot'] = acidity_total(oot)\n",
        "\n",
        "test['ratio_density_alcohol'] = ratio_density_alcohol(test)\n",
        "oot['ratio_density_alcohol'] = ratio_density_alcohol(oot)\n",
        "\n",
        "#test['ratio_acidity_pH'] = ratio_acidity_pH(test)\n",
        "#oot['ratio_acidity_pH'] = ratio_acidity_pH(oot)\n",
        "\n",
        "test = one_hot_encoding(test, ['citric_cat'])\n",
        "oot = one_hot_encoding(oot, ['citric_cat'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEc4UcGcEFiz"
      },
      "source": [
        "Now, all the **data cleaning for modeling is over**! The train_smote dataset is ready and the dataset test & oot received the same treatment than the train dataset.\n",
        "\n",
        "We can now try some algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnFFV_47ELRt"
      },
      "source": [
        "# Modeling\n",
        "In this chapter we will see the differents algorithms in order to predict the wine quality. Moreover, we will measure the results of our predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbCkItNdEYN-"
      },
      "source": [
        "# Logistic Regression with statsmodels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtkgJMAmEUD9"
      },
      "source": [
        "import statsmodels.api as stms\n",
        "# Be careful with the dummy features because the logistic regression is sensitive\n",
        "# to the full rank column condition\n",
        "\n",
        "logit_model = stms.Logit(train_smote['Y'], train_smote[final_list_features].iloc[:,0:-1])\n",
        "result_logit = logit_model.fit()\n",
        "\n",
        "print(result_logit.summary2())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldnYz9bhD_n9"
      },
      "source": [
        "# Here we can see that LLR p-value:1.3770e-120 -> It is <0.05 so the overall model is significant to predict the quality\n",
        "# However, fixed acidity, citric acid and alcohol have a P>|z| <0.05 -> they are not significant in the model\n",
        "\n",
        "# We need to delete them. \n",
        "# BE CAREFUL! You have to delete them on an iterative way from the one having the highest pvalue to the one having the lowest.\n",
        "\n",
        "logit_model = stms.Logit(train_smote['Y'], train_smote[['volatile acidity', \n",
        "                                                       'citric acid', 'chlorides',\n",
        "                                                       'total sulfur dioxide', 'density', \n",
        "                                                       'sulphates', 'alcohol',\n",
        "                                                       'ratio_density_alcohol', \n",
        "                                                       'citric_cat_A']])\n",
        "result_logit = logit_model.fit()\n",
        "\n",
        "print(result_logit.summary2())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vZ2X69qFQ33"
      },
      "source": [
        "logit_model = stms.Logit(train_smote['Y'], train_smote[['volatile acidity', \n",
        "                                                       'citric acid', 'chlorides',\n",
        "                                                       'total sulfur dioxide', 'density', \n",
        "                                                       'sulphates',\n",
        "                                                       'ratio_density_alcohol', \n",
        "                                                       'citric_cat_A']])\n",
        "result_logit = logit_model.fit()\n",
        "\n",
        "print(result_logit.summary2())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shxdKi8ZFIkF"
      },
      "source": [
        "logit_model = stms.Logit(train_smote['Y'], train_smote[['volatile acidity', \n",
        "                                                       'chlorides',\n",
        "                                                       'total sulfur dioxide', 'density', \n",
        "                                                       'sulphates',\n",
        "                                                       'ratio_density_alcohol', \n",
        "                                                       'citric_cat_A']])\n",
        "result_logit = logit_model.fit()\n",
        "\n",
        "print(result_logit.summary2())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P_xoKraGEpF"
      },
      "source": [
        "# Here, everything seems to work well.\n",
        "# However, there is a coefficient who takes all the information.\n",
        "# We know that it is equals to density / alcohol -> let's delete it and add alcohol again"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoiExNsSGJ1k"
      },
      "source": [
        "logit_model = stms.Logit(train_smote['Y'], train_smote[['volatile acidity', \n",
        "                                                       'chlorides',\n",
        "                                                       'total sulfur dioxide', 'density', \n",
        "                                                       'sulphates', 'alcohol',\n",
        "                                                        \n",
        "                                                       'citric_cat_A']])\n",
        "result_logit = logit_model.fit()\n",
        "\n",
        "print(result_logit.summary2())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrJFbntfGN4z"
      },
      "source": [
        "# Now the coefficients are more homogeneous and the AIC/BIC have almost the same values as before\n",
        "# So we can keep this modeling.\n",
        "\n",
        "# Now, let's predict them on the test & oot\n",
        "\n",
        "test = pd.concat([test, pd.Series(result_logit.predict(test[['volatile acidity', 'chlorides','total sulfur dioxide', \n",
        "                           'density', 'sulphates', 'alcohol', 'citric_cat_A']]), name='Predict_statsmodels')], axis=1)\n",
        "\n",
        "oot = pd.concat([oot, pd.Series(result_logit.predict(oot[['volatile acidity', 'chlorides','total sulfur dioxide', \n",
        "                           'density', 'sulphates', 'alcohol', 'citric_cat_A']]), name='Predict_statsmodels')], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3XkfUieGSzo"
      },
      "source": [
        "def confusion_matrix(dataset, true_label, predicted_probas, number_to_consider):\n",
        "  pos = np.ones(number_to_consider,)\n",
        "  neg = np.zeros(dataset.shape[0] - number_to_consider,)\n",
        "  dataset = dataset.sort_values(by=predicted_probas, ascending=False)\n",
        "  dataset['Label_predicted_statsmodels'] = np.concatenate((pos, neg), axis=0)\n",
        "  del(pos, neg, number_to_consider)\n",
        "  output = pd.crosstab(dataset['Label_predicted_statsmodels'], dataset[true_label])\n",
        "  print('\\nTarget rate: '+ str(np.round(dataset[true_label].sum()/len(dataset), 2)))\n",
        "  print('\\nPrecision: ' + str(np.round((output[1][1])/(output[0][1]+output[1][1]), 2)))\n",
        "  print('\\nRecall: ' + str(np.round((output[1][1])/(output[1][0]+output[1][1]), 2)))\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNdz_MQ3GVz0"
      },
      "source": [
        "confusion_matrix(test, 'Y', 'Predict_statsmodels',33)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un6pRzjjGb4T"
      },
      "source": [
        "confusion_matrix(oot, 'Y', 'Predict_statsmodels', 111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ikw7D6GhEdKN"
      },
      "source": [
        "# Logistic regression with sklearn | Ridge & Lasso"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_skl = LogisticRegression(random_state=893717398, max_iter=300).fit(train_smote[['volatile acidity', \n",
        "                                                       'chlorides', 'total sulfur dioxide', \n",
        "                                                       'density', 'sulphates', \n",
        "                                                       'alcohol', 'citric_cat_A']], \n",
        "                                                        train_smote['Y'])\n",
        "\n",
        "# Here I added .reset_index() in order to avoid some mismatch with the predict\n",
        "test.reset_index(inplace=True)\n",
        "oot.reset_index(inplace=True)\n",
        "\n",
        "# Given the fact we use sklearn the prediction is a 2D array, so I add [:,1] in order \n",
        "# to keep only the probability of 1 in order to suit the confusion_matrix function\n",
        "test = pd.concat([test, pd.Series(lr_skl.predict_proba(test[['volatile acidity', \n",
        "                                                             'chlorides', \n",
        "                                                             'total sulfur dioxide', \n",
        "                                                             'density', 'sulphates', \n",
        "                                                             'alcohol', 'citric_cat_A']])[:,1], \n",
        "                                  name='Predict_sklearn_reglog_ridge')], axis=1)\n",
        "\n",
        "oot = pd.concat([oot, pd.Series(lr_skl.predict_proba(oot[['volatile acidity', \n",
        "                                                             'chlorides', \n",
        "                                                             'total sulfur dioxide', \n",
        "                                                             'density', 'sulphates', \n",
        "                                                             'alcohol', 'citric_cat_A']])[:,1], \n",
        "                                  name='Predict_sklearn_reglog_ridge')], axis=1)\n"
      ],
      "metadata": {
        "id": "d7-5oq-Sm0Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(test, 'Y', 'Predict_sklearn_reglog_ridge',33)"
      ],
      "metadata": {
        "id": "ZLhOwVdhpjB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(oot, 'Y', 'Predict_sklearn_reglog_ridge', 111)"
      ],
      "metadata": {
        "id": "RKLjZ5Lwpovk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On this first test it appears that the Ridge regression is less powerful than the logistic one."
      ],
      "metadata": {
        "id": "_6AprYVEpsEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_skl_lasso = LogisticRegression(random_state=893717398, \n",
        "                                  max_iter=300, \n",
        "                                  penalty='l1',\n",
        "                                  solver='liblinear').fit(train_smote[['volatile acidity', \n",
        "                                                       'chlorides', 'total sulfur dioxide', \n",
        "                                                       'density', 'sulphates', \n",
        "                                                       'alcohol', 'citric_cat_A']], \n",
        "                                                        train_smote['Y'])\n",
        "\n",
        "test = pd.concat([test, pd.Series(lr_skl_lasso.predict_proba(test[['volatile acidity', \n",
        "                                                             'chlorides', \n",
        "                                                             'total sulfur dioxide', \n",
        "                                                             'density', 'sulphates', \n",
        "                                                             'alcohol', 'citric_cat_A']])[:,1], \n",
        "                                  name='Predict_sklearn_reglog_lasso')], axis=1)\n",
        "\n",
        "oot = pd.concat([oot, pd.Series(lr_skl_lasso.predict_proba(oot[['volatile acidity', \n",
        "                                                             'chlorides', \n",
        "                                                             'total sulfur dioxide', \n",
        "                                                             'density', 'sulphates', \n",
        "                                                             'alcohol', 'citric_cat_A']])[:,1], \n",
        "                                  name='Predict_sklearn_reglog_lasso')], axis=1)\n"
      ],
      "metadata": {
        "id": "L-G6r85XptH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(test, 'Y', 'Predict_sklearn_reglog_lasso',33)"
      ],
      "metadata": {
        "id": "DHVxit_Cpxct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(oot, 'Y', 'Predict_sklearn_reglog_lasso', 111)"
      ],
      "metadata": {
        "id": "eyEUNvH1p1Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that in this use-case the classical logistic regression has a better predictive capacity even if it is marginal."
      ],
      "metadata": {
        "id": "JmNdt9Tzp9Ps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree with sklearn\n",
        "Now we will see a new family of algorithm: trees The central element of this family is the decision tree.\n",
        "\n",
        "The most used decision tree algorithm is CART."
      ],
      "metadata": {
        "id": "OkjjY6uZqCgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# By default sklearn will create a tree where each leave is pure -> no misclassification\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "\n",
        "dt_skl = tree.DecisionTreeClassifier(random_state=89371)\n",
        "dt_skl.fit(train_smote[final_list_features].iloc[:,0:-1], train_smote['Y'])\n",
        "\n",
        "plt.figure(figsize=(25,12))\n",
        "tree.plot_tree(dt_skl, fontsize=6)\n",
        "plt.show()\n",
        "\n",
        "# BIG problem of overfitting\n",
        "\n",
        "# Solution is pruning"
      ],
      "metadata": {
        "id": "27LucBzWqTCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_skl = DecisionTreeClassifier(random_state=893717398)\n",
        "path_tree = dt_skl.cost_complexity_pruning_path(train_smote[final_list_features].iloc[:,0:-1], train_smote['Y'])\n",
        "ccp_alphas, impurities = path_tree.ccp_alphas, path_tree.impurities\n",
        "\n",
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    dt_skl = DecisionTreeClassifier(random_state=893717398, ccp_alpha=ccp_alpha)\n",
        "    dt_skl.fit(train_smote[final_list_features].iloc[:,0:-1], train_smote['Y'])\n",
        "    clfs.append(dt_skl)\n",
        "\n",
        "train_scores_dt_skl = [dt_skl.score(train_smote[final_list_features].iloc[:,0:-1], train_smote['Y']) for dt_skl in clfs]\n",
        "test_scores_dt_skl = [dt_skl.score(test[final_list_features].iloc[:,0:-1], test['Y']) for dt_skl in clfs]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_xlabel(\"alpha\")\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
        "ax.plot(ccp_alphas, train_scores_dt_skl, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n",
        "ax.plot(ccp_alphas, test_scores_dt_skl, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9EtpujnQqa-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_best_alpha = pd.concat([pd.Series(ccp_alphas), pd.Series(test_scores_dt_skl)], axis=1)\n",
        "get_best_alpha.columns = ['alphas', 'accuracy']\n",
        "get_best_alpha = get_best_alpha.loc[get_best_alpha['accuracy'].idxmax()][0]\n",
        "get_best_alpha"
      ],
      "metadata": {
        "id": "unHtixUeqeVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_skl = tree.DecisionTreeClassifier(random_state=89371, ccp_alpha=get_best_alpha)\n",
        "dt_skl.fit(train_smote[final_list_features].iloc[:,0:-1], train_smote['Y'])\n",
        "\n",
        "plt.figure(figsize=(25,12))\n",
        "tree.plot_tree(dt_skl, fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B8sSwlnlqh-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.concat([test, pd.Series(dt_skl.predict_proba(test[final_list_features].iloc[:,0:-1])[:,1], \n",
        "                                  name='Predict_sklearn_dt')], axis=1)\n",
        "oot = pd.concat([oot, pd.Series(dt_skl.predict_proba(oot[final_list_features].iloc[:,0:-1])[:,1], \n",
        "                                  name='Predict_sklearn_dt')], axis=1)"
      ],
      "metadata": {
        "id": "QK3dEm5TqkwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(test, 'Y', 'Predict_sklearn_dt',33)"
      ],
      "metadata": {
        "id": "Z_fC2jUQqqQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(oot, 'Y', 'Predict_sklearn_dt', 111)"
      ],
      "metadata": {
        "id": "5y37nkmHqu7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_lfvKhpEjMp"
      },
      "source": [
        "# Random forest with sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML1n1zyDEpSs"
      },
      "source": [
        "rf_skl = RandomForestClassifier(bootstrap=True, # resampling of the database for each tree\n",
        "                                       class_weight=None, #we already resampled before with SMOTE\n",
        "                                       criterion='gini', #gini or entropy, not very important\n",
        "                                       max_depth=5, #take a number between 5&8 depending of the size of the database\n",
        "                                       max_features='auto', #number of featuer selected by tree\n",
        "                                       max_leaf_nodes=None, #not important to change\n",
        "                                       min_samples_leaf=30, #Because we have a small database, 150 is good\n",
        "                                       n_estimators=500, #Number of trees\n",
        "                                       n_jobs=-1, #We take all the processors -1 that we have\n",
        "                                       random_state=893717398).fit(\n",
        "                                           train_smote[final_list_features].iloc[:,0:-1], \n",
        "                                           train_smote['Y'])\n",
        "\n",
        "test = pd.concat([test, pd.Series(rf_skl.predict_proba(test[final_list_features].iloc[:,0:-1])[:,1], \n",
        "                                  name='Predict_sklearn_rf')], axis=1)\n",
        "oot = pd.concat([oot, pd.Series(rf_skl.predict_proba(oot[final_list_features].iloc[:,0:-1])[:,1], \n",
        "                                  name='Predict_sklearn_rf')], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(test, 'Y', 'Predict_sklearn_rf',33)"
      ],
      "metadata": {
        "id": "UonFDjx0q4zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(oot, 'Y', 'Predict_sklearn_rf', 111)"
      ],
      "metadata": {
        "id": "elstgis9q7MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-qdNQDaEqmr"
      },
      "source": [
        "# Gradient Boosting with xgboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxXIGiRvEv0G"
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "parameters = {\n",
        "     \"eta\"    : [0.05, 0.15, 0.25, 0.30 ] ,\n",
        "     \"max_depth\"        : [ 3, 4, 5],\n",
        "     \"min_child_weight\" : [ 1, 3, 5],\n",
        "     \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3],\n",
        "     \"colsample_bytree\" : [ 0.3, 0.5 , 0.7 ]\n",
        "     }\n",
        "\n",
        "xgb_grid = GridSearchCV(xgb_clf,\n",
        "                    parameters, n_jobs=-1,\n",
        "                    scoring=\"neg_log_loss\",\n",
        "                    cv=3).fit(train_smote[final_list_features].iloc[:,0:-1], train_smote['Y'])\n",
        "\n",
        "test = pd.concat([test, pd.Series(xgb_grid.predict_proba(test[final_list_features].iloc[:,0:-1])[:,1], \n",
        "                                  name='Predict_xgb_grid')], axis=1)\n",
        "oot = pd.concat([oot, pd.Series(xgb_grid.predict_proba(oot[final_list_features].iloc[:,0:-1])[:,1], \n",
        "                                  name='Predict_xgb_grid')], axis=1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(test, 'Y', 'Predict_xgb_grid',33)"
      ],
      "metadata": {
        "id": "psQTcbp6rFBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(oot, 'Y', 'Predict_xgb_grid', 111)"
      ],
      "metadata": {
        "id": "cknrAurhrHgh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}